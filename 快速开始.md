# 🚀 快速开始 - 对抗攻击演示

## 第一步：理解代码结构

这个代码库包含多种对抗攻击方法的实现：

```
adversarial-attacks/
├── attack.py          # 攻击基类（所有攻击方法都继承自此类）
├── fgsm.py           # FGSM攻击（最简单快速）
├── pgd.py            # PGD攻击（迭代攻击）
├── mifgsm.py         # MI-FGSM攻击（带动量）
├── tifgsm.py         # TI-FGSM攻击（平移不变）
├── mi_smi.py         # MI-SMI攻击（空间动量）
└── ...               # 其他攻击方法
```

## 第二步：运行简单演示

### 方法1：使用 simple_demo.py（推荐新手）

```bash
python simple_demo.py
```

这个脚本会：
1. 加载一个预训练的ResNet模型
2. 创建一个测试图像
3. 使用FGSM生成对抗样本
4. 显示对比结果（原始图像 vs 对抗样本 vs 扰动）

### 方法2：使用 demo_attack.py（对比多种方法）

```bash
python demo_attack.py
```

这个脚本会：
1. 测试多种攻击方法（FGSM, PGD, MI-FGSM等）
2. 对比不同方法的效果
3. 生成详细的可视化结果

## 第三步：理解输出

运行后会看到：

1. **控制台输出**：
   - 原始图像的预测结果
   - 对抗样本的预测结果
   - 攻击是否成功
   - 扰动大小统计

2. **可视化图像**（保存为PNG文件）：
   - 左：原始图像及其预测
   - 中：对抗样本及其预测
   - 右：添加的扰动（放大显示）

## 第四步：调整参数

在代码中修改 `epsilon` 参数来调整攻击强度：

```python
config = {
    "epsilon": 0.05  # 增大这个值 = 更强的攻击，但图像变化更明显
                     # 减小这个值 = 更隐蔽的攻击，但可能失败
}
```

**建议值**：
- `0.01` - 非常小的扰动，攻击可能失败
- `0.03` - 中等扰动，平衡攻击效果和隐蔽性
- `0.05` - 较大的扰动，攻击成功率高
- `0.1` - 很大的扰动，图像变化明显

## 第五步：使用自己的图像

修改 `simple_demo.py` 中的图像加载部分：

```python
# 替换这部分
test_image = torch.rand(1, 3, 224, 224)

# 改为加载真实图像
from PIL import Image
import torchvision.transforms as transforms

transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])

image = Image.open("your_image.jpg")
test_image = transform(image).unsqueeze(0)
```

## 常见问题

### Q: 导入错误怎么办？
A: 确保所有文件在同一目录下，并且已安装依赖：
```bash
pip install torch torchvision matplotlib numpy pillow
```

### Q: 攻击总是失败？
A: 尝试增大 `epsilon` 值，或者使用更强的攻击方法（如PGD或MI-FGSM）

### Q: 如何攻击自己的模型？
A: 替换模型加载部分：
```python
# 加载你的模型
model = your_model()
model.load_state_dict(torch.load('your_model.pth'))
model.eval().to(device)
```

### Q: 如何评估攻击效果？
A: 计算攻击成功率：
```python
success_rate = (original_pred != adversarial_pred).float().mean()
print(f"攻击成功率: {success_rate:.2%}")
```

## 下一步

1. 阅读 `使用指南.md` 了解详细原理
2. 尝试不同的攻击方法
3. 调整参数观察效果变化
4. 在自己的模型上测试

