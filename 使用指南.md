# 对抗攻击方法使用指南

## 📚 什么是对抗攻击？

对抗攻击（Adversarial Attack）是一种针对机器学习模型的攻击技术。通过在原始输入上添加微小的、人眼几乎无法察觉的扰动，使得模型产生错误的预测结果。

### 核心概念

1. **原始样本（Clean Sample）**：正常的输入图像
2. **对抗样本（Adversarial Sample）**：添加了微小扰动后的图像
3. **扰动（Perturbation）**：添加到原始图像上的微小变化
4. **目标攻击 vs 非目标攻击**：
   - **非目标攻击**：只要让模型分类错误即可
   - **目标攻击**：让模型预测为指定的错误类别

## 🎯 本代码库包含的攻击方法

### 1. **FGSM** (Fast Gradient Sign Method)
- **特点**：快速、单步攻击
- **原理**：沿着损失函数的梯度方向，一步添加扰动
- **适用场景**：快速测试模型鲁棒性

### 2. **PGD** (Projected Gradient Descent)
- **特点**：迭代攻击，更强大
- **原理**：多步迭代，每次在扰动范围内更新
- **适用场景**：评估模型防御能力

### 3. **MIFGSM** (Momentum Iterative FGSM)
- **特点**：带动量的迭代攻击
- **原理**：使用动量来稳定梯度方向，提高攻击成功率
- **适用场景**：需要更强攻击效果的场景

### 4. **TIFGSM** (Translation-Invariant FGSM)
- **特点**：对平移变换鲁棒
- **原理**：使用卷积核平滑梯度，提高迁移性
- **适用场景**：黑盒攻击（攻击其他模型）

### 5. **MISMI** (Momentum Iterative with Spatial Momentum)
- **特点**：结合动量和空间变换
- **原理**：使用空间动量和输入变换增强攻击
- **适用场景**：需要高迁移性的攻击

### 6. **其他方法**
- **DIFGSM**：带输入多样性的FGSM
- **SMIFGSM**：空间动量FGSM
- **DMIFGSM**：深度动量FGSM

## 🚀 快速开始

### 基本使用流程

```python
# 1. 导入攻击方法
from fgsm import FGSM

# 2. 准备模型和数据
model = your_model  # 你的分类模型
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
images = your_images  # 输入图像 (batch_size, channels, height, width)
labels = your_labels  # 真实标签

# 3. 创建攻击对象
config = {"epsilon": 0.03}  # 扰动大小
attack = FGSM(model=model, device=device, IsTargeted=False, config=config)

# 4. 生成对抗样本
adversarial_images = attack.generate(xs=images, ys=labels)

# 5. 测试攻击效果
original_pred = model(images).argmax(1)
adversarial_pred = model(adversarial_images).argmax(1)
success_rate = (original_pred != adversarial_pred).float().mean()
```

## 📊 参数说明

### 通用参数

- **epsilon (ε)**：最大扰动范围，通常为 0.01-0.1
  - 值越大，攻击越强，但图像变化越明显
  - 值越小，图像变化越小，但攻击可能失败

- **IsTargeted**：是否为目标攻击
  - `False`：非目标攻击（只要分类错误即可）
  - `True`：目标攻击（需要预测为指定类别）

### 迭代攻击特有参数

- **num_steps**：迭代次数（默认 10-15）
  - 更多迭代通常带来更强攻击，但计算时间更长

- **eps_iter**：每次迭代的步长
  - 通常为 `epsilon / num_steps`

- **decay_factor**：动量衰减因子（默认 1.0）
  - 用于稳定梯度方向

## 💡 使用建议

1. **从简单开始**：先用 FGSM 快速测试
2. **调整 epsilon**：从 0.01 开始，逐步增加
3. **对比不同方法**：比较不同攻击方法的效果
4. **可视化结果**：查看原始图像和对抗样本的差异
5. **评估鲁棒性**：用攻击成功率评估模型鲁棒性

## ⚠️ 注意事项

1. 确保模型处于 `eval()` 模式
2. 输入图像需要归一化到 [0, 1] 范围
3. 标签需要从 0 开始编号
4. 使用 GPU 可以显著加速攻击过程

